{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ReadingData.ipynb\n",
      "importing Jupyter notebook from model.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import Ipynb_importer\n",
    "from ReadingData import DataReader, Metapath2vecDataset\n",
    "from model import SkipGramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metapath2VecTrainer:\n",
    "    def __init__(self, file, min_count, window_size, batch_size, output_file, dim, iterations, initial_lr):\n",
    "        self.data = DataReader(file, min_count)\n",
    "        dataset = Metapath2vecDataset(self.data, window_size)\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                     shuffle=True, num_workers=4, collate_fn=dataset.collate)\n",
    "\n",
    "        self.output_file_name = output_file\n",
    "        self.emb_size = len(self.data.word2id)\n",
    "        self.emb_dimension = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "        self.initial_lr = initial_lr #learning rate\n",
    "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        if self.use_cuda:\n",
    "            self.skip_gram_model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for iteration in range(self.iterations):\n",
    "            print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
    "            optimizer = optim.SparseAdam(self.skip_gram_model.parameters(), lr=self.initial_lr)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, sample_batched in enumerate(tqdm(self.dataloader)):\n",
    "\n",
    "                if len(sample_batched[0]) > 1:\n",
    "                    pos_u = sample_batched[0].to(self.device)\n",
    "                    pos_v = sample_batched[1].to(self.device)\n",
    "                    neg_v = sample_batched[2].to(self.device)\n",
    "\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
    "                    if i > 0 and i % 500 == 0:\n",
    "                        print(\" Loss: \" + str(running_loss))\n",
    "\n",
    "            self.skip_gram_model.save_embedding(self.data.id2word, self.output_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../output/output_path.txt'\n",
    "min_count = 5   #单词频率截断\n",
    "window_size = 5  #窗口大小\n",
    "batch_size = 128  #批大小\n",
    "output_file = '../output/embeddings.txt' #embedding输出路径\n",
    "dim = 128  # embedding维度\n",
    "iterations = 10  # 循环次数\n",
    "initial_lr = 0.01  #learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings: 20864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linbang/anaconda3/envs/DGL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 92%|█████████▏| 502/546 [01:13<00:06,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 2.9129858167833333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:19<00:00,  6.89it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [01:13<00:06,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 2.786793152517554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:19<00:00,  6.89it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [01:07<00:06,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 2.7567128685322677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:13<00:00,  7.43it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [01:12<00:07,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 2.788184064071736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:20<00:00,  6.80it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [01:01<00:04, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 2.8321075558231787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:07<00:00,  8.14it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 503/546 [00:58<00:03, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 2.940539925790936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:02<00:00,  8.70it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [01:08<00:05,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 3.0533957102930245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:14<00:00,  7.35it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [01:08<00:05,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 3.2011887262040517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:13<00:00,  7.40it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [00:59<00:04, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 3.310098170198761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:04<00:00,  8.49it/s]\n",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 502/546 [00:59<00:04, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 3.4719487614076523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [01:03<00:00,  8.60it/s]\n"
     ]
    }
   ],
   "source": [
    "m2v = Metapath2VecTrainer(file, min_count, window_size, batch_size, output_file, dim, iterations, initial_lr)\n",
    "m2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(DGL)",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
