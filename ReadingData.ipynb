{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    NEGATIVE_TABLE_SIZE = 1e8\n",
    "    \n",
    "    def __init__(self,file, min_count):\n",
    "        self.inputFileName = file\n",
    "        self.negatives = []\n",
    "        self.discards = []\n",
    "        self.negpos = 0\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.sentences_count = 0\n",
    "        self.token_count = 0\n",
    "        self.word_frequency = dict()\n",
    "        \n",
    "        self.read_words(min_count)\n",
    "        self.initTableNegatives()\n",
    "        self.initTableDiscards()\n",
    "    \n",
    "    def read_words(self,min_count):\n",
    "        #词频表\n",
    "        f = open(self.inputFileName, 'r')\n",
    "        word_frequency = dict()\n",
    "        for line in f.readlines():\n",
    "            line = line.split(',')\n",
    "            self.sentences_count += 1\n",
    "            for word in line:\n",
    "                self.token_count += 1\n",
    "                word_frequency[word] = word_frequency.get(word,0) + 1\n",
    "        f.close()\n",
    "        #建立词汇表\n",
    "        wid = 0\n",
    "        for w, c in word_frequency.items():\n",
    "            if c < min_count: #删除出现频率比较小的词汇\n",
    "                continue\n",
    "            self.word2id[w] = wid\n",
    "            self.id2word[wid] = w\n",
    "            self.word_frequency[wid] = c\n",
    "            wid += 1\n",
    "\n",
    "        self.word_count = len(self.word2id)\n",
    "        print(\"Total embeddings: \" + str(len(self.word2id)))\n",
    "        \n",
    "    def initTableDiscards(self):\n",
    "        # 为负采样建立frequency表\n",
    "        t = 0.0001\n",
    "        f = np.array(list(self.word_frequency.values())) / self.token_count\n",
    "        self.discards = np.sqrt(t / f) + (t / f)\n",
    "\n",
    "    def initTableNegatives(self):\n",
    "        # get a table for negative sampling, if word with index 2 appears twice, then 2 will be listed\n",
    "        # in the table twice.\n",
    "        pow_frequency = np.array(list(self.word_frequency.values())) ** 0.75\n",
    "        words_pow = sum(pow_frequency)\n",
    "        ratio = pow_frequency / words_pow\n",
    "        count = np.round(ratio * DataReader.NEGATIVE_TABLE_SIZE)\n",
    "        for wid, c in enumerate(count):\n",
    "            self.negatives += [wid] * int(c)\n",
    "        self.negatives = np.array(self.negatives)\n",
    "        np.random.shuffle(self.negatives)\n",
    "        self.sampling_prob = ratio\n",
    "\n",
    "    def getNegatives(self, target, size):  # TODO check equality with target\n",
    "        response = self.negatives[self.negpos:self.negpos + size]\n",
    "        self.negpos = (self.negpos + size) % len(self.negatives)\n",
    "        if len(response) != size:\n",
    "            return np.concatenate((response, self.negatives[0:self.negpos]))\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metapath2vecDataset(Dataset):\n",
    "    def __init__(self, data, window_size):\n",
    "        # read in data, window_size and input filename\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.input_file = open(data.inputFileName,'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of walks\n",
    "        return self.data.sentences_count\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return the list of pairs (center, context, 5 negatives)\n",
    "        while True:\n",
    "            line = self.input_file.readline()\n",
    "            if not line:\n",
    "                self.input_file.seek(0, 0)\n",
    "                line = self.input_file.readline()\n",
    "\n",
    "            if len(line) > 1:\n",
    "                words = line.split(',')\n",
    "\n",
    "                if len(words) > 1:\n",
    "                    word_ids = [self.data.word2id[w] for w in words if\n",
    "                                w in self.data.word2id and np.random.rand() < self.data.discards[self.data.word2id[w]]]\n",
    "\n",
    "                    pair_catch = []\n",
    "                    for i, u in enumerate(word_ids):\n",
    "                        for j, v in enumerate(\n",
    "                                word_ids[max(i - self.window_size, 0):i + self.window_size]):\n",
    "                            assert u < self.data.word_count\n",
    "                            assert v < self.data.word_count\n",
    "                            if i == j:\n",
    "                                continue\n",
    "                            pair_catch.append((u, v, self.data.getNegatives(v,5)))\n",
    "                    return pair_catch\n",
    "                \n",
    "    @staticmethod\n",
    "    def collate(batches):\n",
    "        all_u = [u for batch in batches for u, _, _ in batch if len(batch) > 0]\n",
    "        all_v = [v for batch in batches for _, v, _ in batch if len(batch) > 0]\n",
    "        all_neg_v = [neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0]\n",
    "        return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = '../output/output_path.txt'\n",
    "#min_count = 5\n",
    "#window_size = 5\n",
    "#batch_size = 1\n",
    "#num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = DataReader(file, min_count)\n",
    "#dataset = Metapath2vecDataset(data, window_size)\n",
    "#dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "#                        shuffle=True, num_workers=num_workers, collate_fn=dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,bat in enumerate(dataloader):\n",
    "#    print(bat)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(DGL)",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
